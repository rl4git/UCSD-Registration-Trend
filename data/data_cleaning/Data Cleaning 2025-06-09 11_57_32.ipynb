{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62711199-4cb8-4bc7-8eef-9aa16b85bfd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Required Variables\n",
    "Since I am using Databricks Community Edition, I cannot use `dbutils.secrets` to store secret key, or mount S3 bucket using `deutils.fs.mount`. Therefore, I have to declare the following variables explicitly.\n",
    "- AWS_ACCESS_KEY\n",
    "- AWS_SECRET_KEY\n",
    "- BUCKET_NAME\n",
    "- REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bca1d9e-3f5b-48ee-9db4-27787f8471d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY = \"\"\n",
    "AWS_SECRET_KEY = \"\"\n",
    "BUCKET_NAME = \"\"\n",
    "REGION = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2d0ed9-0c89-4f4c-8d97-cb71f2fe4015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Spark Instance And Test S3 Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b254abc-4b12-43d0-bbb8-aa5044cf2ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "# spark.conf.set(\"fs.s3a.endpoint\", f\"s3.{REGION}.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8d83a5c-9fca-44ab-be8b-77f7ba3aa3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+--------+------+--------------------+---------+--------+-----+-----------+\n|         time|subj_course_id|sec_code|sec_id|                prof|available|waitlist|total|enrolled_ct|\n+-------------+--------------+--------+------+--------------------+---------+--------+-----+-----------+\n|1704976213683|        AAS 10|     A01|303104|Butler; Elizabeth...|        1|       4|   34|         33|\n|1704976213683|        AAS 10|     A02|303108|Butler; Elizabeth...|        1|       6|   34|         33|\n|1704976214231|        AAS 11|     A01|303127|Butler; Elizabeth...|        1|       0|   34|         33|\n+-------------+--------------+--------+------+--------------------+---------+--------+-----+-----------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "base_path = f\"s3a://{BUCKET_NAME}/ucsd\"\n",
    "base_path_raw = f\"{base_path}/raw\"\n",
    "base_path_cleaned = f\"{base_path}/cleaned\"\n",
    "base_path_final_table = f\"{base_path}/final_table\"\n",
    "base_path_final = f\"{base_path}/final\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.csv(f\"{base_path_raw}/2024Winter\", header=True, inferSchema=True)\n",
    "    df.show(3)\n",
    "except Exception as e:\n",
    "    print(f\"Table read failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa4deb3-2b6e-4915-b3d8-c7e13a483ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n|prof                        |\n+----------------------------+\n|Butler; Elizabeth Annette   |\n|Butler; Elizabeth Annette   |\n|Butler; Elizabeth Annette   |\n|Butler; Elizabeth Annette   |\n|Wade; Jon P                 |\n|Staff                       |\n|Staff                       |\n|Zak; Alexander & Wade; Jon P|\n|Vespa; Emanuel Ignacio      |\n|Myerston Santana; Jacobo    |\n|Rao; Ramesh R               |\n|Lovett-Barron; Matthew Rod  |\n|Bronstein; Phoebe Malan     |\n|Danks; David J              |\n|Vespa; Emanuel Ignacio      |\n|Theodorakis; Emmanuel       |\n|Amir; On                    |\n|Schmidt; Thomas Rainer      |\n|Treichler; Emily Brockway   |\n|Root; Cory Matthew          |\n+----------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select('prof').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca08d8d3-1072-4c37-aa71-1ca4ddb2c66f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import Necessory Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "816669dd-8517-45e9-a584-91a5cf97e113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, sum, when, lit\n",
    "from pyspark.sql.functions import sum, avg, max, min, count, countDistinct, first, last, mean, stddev, collect_list, collect_set, approx_count_distinct, expr\n",
    "from pyspark.sql.functions import col, from_unixtime, to_timestamp, date_format, row_number\n",
    "from pyspark.sql.functions import split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "986fd9d2-fe64-4a10-a182-7e902c760ba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Check Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f171ad-aa76-4a54-a016-f753d506f8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Check and analyze the raw data\n",
    "For this part, I strongluy recommend to reference my another data-analyze project, which cleaned and analyzed the UCSD registration data in detail: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7894721-108f-4c34-a64b-4ce529ede92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define and store quarter information.\n",
    "- `year_quarter`: list includes the year - quarter registration information that we want to clean and store.\n",
    "- `passtag`: list includes the tag for each pass time, this will be the key of hte passtime. This will also be the horizontal axis of the final graph.\n",
    "- `passtime`: actual registration date of each quarter, corresponds to the value in apsstag. \n",
    "\n",
    "Generate a json data file:\n",
    "- key: `year-quarter`\n",
    "- value: `passtag` : `passtime` for corresponding quarter.\n",
    "- stored: `s3/ucsd/final/passtimes.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c9b274-e0a7-481c-962c-281c604e8827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2705 bytes.\n{\n  \"2024-Winter\": {\n    \"Prior\": \"2023-11-13\",\n    \"First Pass Priorities & Seniors Start\": \"2023-11-14\",\n    \"First Pass Juniors Start\": \"2023-11-16\",\n    \"First Pass Sophmores Start\": \"2023-11-17\",\n    \"First Pass First-Year Start\": \"2023-11-18\",\n    \"Second Pass Priorities & Seniors Start\": \"2023-11-21\",\n    \"Second Pass Juniors Start\": \"2023-11-24\",\n    \"Second Pass Sophmores Start\": \"2023-11-25\",\n    \"Second Pass First-Year Start\": \"2023-11-27\",\n    \"Quarter Start\": \"2024-01-04\",\n    \"A Week After Quarter Start\": \"2024-01-11\"\n  },\n  \"2024-Spring\": {\n    \"Prior\": \"2024-02-16\",\n    \"First Pass Priorities & Seniors Start\": \"2024-02-17\",\n    \"First Pass Juniors Start\": \"2024-02-20\",\n    \"First Pass Sophmores Start\": \"2024-02-21\",\n    \"First Pass First-Year Start\": \"2024-02-22\",\n    \"Second Pass Priorities & Seniors Start\": \"2024-02-26\",\n    \"Second Pass Juniors Start\": \"2024-02-28\",\n    \"Second Pass Sophmores Start\": \"2024-02-29\",\n    \"Second Pass First-Year Start\": \"2024-03-01\",\n    \"Quarter Start\": \"2024-03-27\",\n    \"A Week After Quarter Start\": \"2024-04-04\"\n  },\n  \"2024-Fall\": {\n    \"Prior\": \"2024-05-23\",\n    \"First Pass Priorities & Seniors Start\": \"2024-05-24\",\n    \"First Pass Juniors Start\": \"2024-05-27\",\n    \"First Pass Sophmores Start\": \"2024-05-28\",\n    \"First Pass First-Year Start\": \"2024-05-29\",\n    \"Second Pass Priorities & Seniors Start\": \"2024-06-01\",\n    \"Second Pass Juniors Start\": \"2024-06-04\",\n    \"Second Pass Sophmores Start\": \"2024-06-05\",\n    \"Second Pass First-Year Start\": \"2024-06-06\",\n    \"Quarter Start\": \"2024-09-23\",\n    \"A Week After Quarter Start\": \"2024-09-30\"\n  },\n  \"2025-Winter\": {\n    \"Prior\": \"2024-11-11\",\n    \"First Pass Priorities & Seniors Start\": \"2024-11-12\",\n    \"First Pass Juniors Start\": \"2024-11-14\",\n    \"First Pass Sophmores Start\": \"2024-11-15\",\n    \"First Pass First-Year Start\": \"2024-11-16\",\n    \"Second Pass Priorities & Seniors Start\": \"2024-11-19\",\n    \"Second Pass Juniors Start\": \"2024-11-21\",\n    \"Second Pass Sophmores Start\": \"2024-11-22\",\n    \"Second Pass First-Year Start\": \"2024-11-23\",\n    \"Quarter Start\": \"2025-01-02\",\n    \"A Week After Quarter Start\": \"2025-01-18\"\n  },\n  \"2025-Spring\": {\n    \"Prior\": \"2025-02-14\",\n    \"First Pass Priorities & Seniors Start\": \"2025-02-15\",\n    \"First Pass Juniors Start\": \"2025-02-18\",\n    \"First Pass Sophmores Start\": \"2025-02-19\",\n    \"First Pass First-Year Start\": \"2025-02-20\",\n    \"Second Pass Priorities & Seniors Start\": \"2025-02-24\",\n    \"Second Pass Juniors Start\": \"2025-02-26\",\n    \"Second Pass Sophmores Start\": \"2025-02-27\",\n    \"Second Pass First-Year Start\": \"2025-02-28\",\n    \"Quarter Start\": \"2025-03-26\",\n    \"A Week After Quarter Start\": \"2025-04-02\"\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "year_quarter = [\"2024-Winter\",\n",
    "        \"2024-Spring\",\n",
    "        \"2024-Fall\",\n",
    "        \"2025-Winter\",\n",
    "        \"2025-Spring\"]\n",
    "passtag = [\n",
    "    'Prior', \\\n",
    "    'First Pass Priorities & Seniors Start', 'First Pass Juniors Start', 'First Pass Sophmores Start', 'First Pass First-Year Start', \\\n",
    "    'Second Pass Priorities & Seniors Start', 'Second Pass Juniors Start', 'Second Pass Sophmores Start', 'Second Pass First-Year Start', \\\n",
    "    'Quarter Start', 'A Week After Quarter Start']\n",
    "passtimes = [\n",
    "  ['2023-11-13', '2023-11-14', '2023-11-16', '2023-11-17', '2023-11-18', \\\n",
    "  '2023-11-21', '2023-11-24', '2023-11-25', '2023-11-27', '2024-01-04', '2024-01-11'],\n",
    "\n",
    "  ['2024-02-16', '2024-02-17', '2024-02-20', '2024-02-21', '2024-02-22', \\\n",
    "  '2024-02-26', '2024-02-28', '2024-02-29', '2024-03-01', '2024-03-27', '2024-04-04'],\n",
    "\n",
    "  ['2024-05-23', '2024-05-24', '2024-05-27', '2024-05-28', '2024-05-29', \\\n",
    "  '2024-06-01', '2024-06-04', '2024-06-05', '2024-06-06', '2024-09-23', '2024-09-30'],\n",
    "\n",
    "  ['2024-11-11', '2024-11-12', '2024-11-14', '2024-11-15', '2024-11-16', \\\n",
    "  '2024-11-19', '2024-11-21', '2024-11-22', '2024-11-23', '2025-01-02', '2025-01-18'],\n",
    "\n",
    "  ['2025-02-14', '2025-02-15', '2025-02-18', '2025-02-19', '2025-02-20', \\\n",
    "  '2025-02-24', '2025-02-26', '2025-02-27', '2025-02-28', '2025-03-26', '2025-04-02'],\n",
    "]\n",
    "\n",
    "assert(len(passtag) == len(passtimes[0]))\n",
    "\n",
    "result = {\n",
    "  quarter: dict(zip(passtag, times))\n",
    "  for quarter, times in zip(year_quarter, passtimes)\n",
    "}\n",
    "\n",
    "# json 最后的储存路径\n",
    "output_path = f\"{base_path_final}/passtimes.json\"\n",
    "# 生成json字符串\n",
    "json_string = json.dumps(result, indent=2)\n",
    "# 将json字符串写入指定文件\n",
    "dbutils.fs.put(output_path, json_string, overwrite=True)\n",
    "file_content = dbutils.fs.head(output_path)\n",
    "print(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0fe2273-9f17-431a-904e-f819eb056fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed DataFrame:\n  year_quarter       Prior First Pass Priorities & Seniors Start  \\\n0  2024-Winter  2023-11-13                            2023-11-14   \n1  2024-Spring  2024-02-16                            2024-02-17   \n2    2024-Fall  2024-05-23                            2024-05-24   \n\n  First Pass Juniors Start First Pass Sophmores Start  \\\n0               2023-11-16                 2023-11-17   \n1               2024-02-20                 2024-02-21   \n2               2024-05-27                 2024-05-28   \n\n  First Pass First-Year Start Second Pass Priorities & Seniors Start  \\\n0                  2023-11-18                             2023-11-21   \n1                  2024-02-22                             2024-02-26   \n2                  2024-05-29                             2024-06-01   \n\n  Second Pass Juniors Start Second Pass Sophmores Start  \\\n0                2023-11-24                  2023-11-25   \n1                2024-02-28                  2024-02-29   \n2                2024-06-04                  2024-06-05   \n\n  Second Pass First-Year Start Quarter Start A Week After Quarter Start  \n0                   2023-11-27    2024-01-04                 2024-01-11  \n1                   2024-03-01    2024-03-27                 2024-04-04  \n2                   2024-06-06    2024-09-23                 2024-09-30  \n\nMelted DataFrame:\n   year_quarter                                passtag   pass_time\n0   2024-Winter                                  Prior  2023-11-13\n1   2024-Spring                                  Prior  2024-02-16\n2     2024-Fall                                  Prior  2024-05-23\n3   2025-Winter                                  Prior  2024-11-11\n4   2025-Spring                                  Prior  2025-02-14\n5   2024-Winter  First Pass Priorities & Seniors Start  2023-11-14\n6   2024-Spring  First Pass Priorities & Seniors Start  2024-02-17\n7     2024-Fall  First Pass Priorities & Seniors Start  2024-05-24\n8   2025-Winter  First Pass Priorities & Seniors Start  2024-11-12\n9   2025-Spring  First Pass Priorities & Seniors Start  2025-02-15\n10  2024-Winter               First Pass Juniors Start  2023-11-16\n11  2024-Spring               First Pass Juniors Start  2024-02-20\n12    2024-Fall               First Pass Juniors Start  2024-05-27\n13  2025-Winter               First Pass Juniors Start  2024-11-14\n14  2025-Spring               First Pass Juniors Start  2025-02-18\n15  2024-Winter             First Pass Sophmores Start  2023-11-17\n16  2024-Spring             First Pass Sophmores Start  2024-02-21\n17    2024-Fall             First Pass Sophmores Start  2024-05-28\n18  2025-Winter             First Pass Sophmores Start  2024-11-15\n19  2025-Spring             First Pass Sophmores Start  2025-02-19\n"
     ]
    }
   ],
   "source": [
    "# 把 pastimes 转为csv储存到S3\n",
    "# 转置dataframe\n",
    "df = pd.DataFrame(result)\n",
    "\n",
    "# Transpose the DataFrame\n",
    "df_transposed = df.T\n",
    "\n",
    "# Reset index to make 'year_quarter' a column\n",
    "df_transposed = df_transposed.reset_index()\n",
    "\n",
    "# Rename the 'index' column to 'year_quarter'\n",
    "df_transposed = df_transposed.rename(columns={'index': 'year_quarter'})\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(\"Transformed DataFrame:\")\n",
    "print(df_transposed.head(3))\n",
    "\n",
    "# Melt the DataFrame to have 'year_quarter', 'passtag', and 'pass_time' as columns\n",
    "df_melted = df_transposed.melt(id_vars=['year_quarter'], var_name='passtag', value_name='pass_time')\n",
    "\n",
    "# Display the melted DataFrame\n",
    "print(\"\\nMelted DataFrame:\")\n",
    "print(df_melted.head(20))\n",
    "\n",
    "# 将df存储到S3\n",
    "passtimes_path = f\"{base_path_final_table}/passtimes/\"\n",
    "columns = [\"year_quarter\", \"passtag\", \"pass_time\"]\n",
    "df_melted_spark = spark.createDataFrame(df_melted, columns).withColumnRenamed(\"pass_time\", \"passtime\")\n",
    "df_melted_spark = df_melted_spark.withColumn(\n",
    "        \"year\",\n",
    "        split(col(\"year_quarter\"), \"-\").getItem(0)\n",
    "    ).withColumn(\n",
    "        \"quarter\",\n",
    "        split(col(\"year_quarter\"), \"-\").getItem(1)\n",
    "    ).drop(\"year_quarter\")\n",
    "\n",
    "df_melted_spark.coalesce(1).write.csv(passtimes_path, header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc68ee3-1ecb-44b6-a567-a9e786f86e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "142a7481-4a2a-4541-83a1-16a5bd12e36d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data cleaning process\n",
    "- Read raw data for each year-quarter\n",
    "- remove null values\n",
    "- translate unix timestamp to readable time, which include date\n",
    "- uniform the professor name for each course-section\n",
    "  - ucsd may change the course professor during the registration, so we only take the final professor.\n",
    "- aggerate the registration value (enrolled, waitlist, etc.) for each course-professor-date. We only need the data for each day.\n",
    "  - enrolled: take the earlist value of the day\n",
    "  - waitlist: take the earlist value of the day\n",
    "  - total: take the earlist value of the day\n",
    "- aggerate each section. Each course taught by each professor may have multiple sections. It is meaningless to keep multiple sections, so aggregate them.\n",
    "  - total: sum\n",
    "  - enrolled: sum\n",
    "  - waitlist: sum\n",
    "- clean the `total` value, only keep the resonable value\n",
    "  - total is a special value in raw data, which include the following values:\n",
    "    - 5 ~ 500 (estimate): this is a resonable value, which indicates the course size.\n",
    "    - 9999: abnormal value, which means the course size has not yet been determined and can be divided into following situations:\n",
    "      - the course size was determined to be a reasonable value later, and we will use this reasonable value as `total` (max value between 5 ~ 1000)\n",
    "      - the course data remains unchanged at 9999. However, for some courses, although their total remains at 9999, their course enrolled data shows reasonable changes (increasing over time). Therefore, for courses with total=9999, their total is marked as -1 and handed over to the backend for processing.\n",
    "- Add new columns:\n",
    "  - `year`: year of the course\n",
    "  - `quarter`: quarter of the course\n",
    "  - `prof_first_name`: professor first name\n",
    "  - `prof_last_name`: professor last name\n",
    "  - `prof_middle_name`: can be null\n",
    "  - `department`: like: `CSE`, `ASS`\n",
    "  - `course_id`: like : `120`, `10`\n",
    "- remove columns `available`, `subj_course_id`, `prof` \n",
    "  - Due to the abnormal value of `total` column, `available` is also untrustworthy, while it can be calculated using `total - enrolled - waitlisted`, it is better to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4008884-a06c-4a26-b23c-74a0e6e0a035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 工具函数\n",
    "# 计数每一列的null值\n",
    "def count_null_values(df):\n",
    "  null_counts = df.select([\n",
    "      sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns\n",
    "  ])\n",
    "  return null_counts\n",
    "\n",
    "# 出区每一列的Null值\n",
    "def remove_null_values(df):\n",
    "  df_without_null = df.na.drop()\n",
    "  return df_without_null\n",
    "\n",
    "# 将time列从UNIX时间戳转为可读时间\n",
    "def transfer_timestamp(df):\n",
    "  df_converted = df.withColumn(\"readable_time\", (col(\"time\")/1000).cast(\"timestamp\"))\n",
    "  df_converted = df_converted.withColumn(\"date\", date_format(col(\"readable_time\"), \"yyyy-MM-dd\"))\n",
    "  return df_converted\n",
    "\n",
    "# 消去不合理值，但是不管total的9999和0，因为数据库会变\n",
    "def remove_weird_values(df):\n",
    "  # df = df.filter(col(\"available\") < 1000)\n",
    "  return df\n",
    "\n",
    "# 消去 prof=Staff 的值\n",
    "# 对于每一门course+secid的组合，UCSD 可能不会在一开始指定教授，而是用Staff代替，\n",
    "# 或者教授人选在中途发生改变，\n",
    "# 在两种情况下，对于 subj_course_id + sec_id 的组合，prof列都发生了变化，即不统一\n",
    "# 因此，对于所有的subj_course_id + sec_id组，以time排序，将prof改为最后一位教授的值\n",
    "def uniform_prof(df):\n",
    "  window_spec = Window.partitionBy(\"subj_course_id\", \"sec_id\").orderBy(\"time\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "  df_with_last_prof = df.withColumn(\"latest_prof\", last(\"prof\", ignorenulls=True).over(window_spec))\n",
    "  df_result = df_with_last_prof.withColumn(\"prof\", col(\"latest_prof\")).drop(\"latest_prof\")\n",
    "\n",
    "  return df_result\n",
    "\n",
    "# 聚合同一门课在每一天的数据\n",
    "# 同一门课+同一个教授+同一个section判定为同一门课\n",
    "# 删去 time，readable_time列\n",
    "# 对于每一天的注册数据：取当天的起始数据（即当天 timestamp 最小的数据\n",
    "# 然后：\n",
    "# 聚合每一个section的课程：注册数据相加\n",
    "# 最后，再次过滤\n",
    "def aggerate_value(df):\n",
    "\n",
    "  # 构造窗口\n",
    "  window_spec = Window.partitionBy(\"subj_course_id\", \"sec_code\", \"sec_id\", \"prof\", \"date\").orderBy(col(\"time\").asc())\n",
    "  # 添加行号\n",
    "  df_temp = df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "  # 过滤，只取当天最早的数据\n",
    "  df_temp = df_temp.filter(col(\"row_number\") == 1).select(\n",
    "    \"subj_course_id\", \"prof\", \"date\",\n",
    "    \"available\", \"total\", \"waitlist\", \"enrolled_ct\"\n",
    "  )\n",
    "\n",
    "  # 聚合不同section\n",
    "  df_temp = df_temp.groupBy(\"subj_course_id\", \"prof\", \"date\").agg(\n",
    "    sum(\"available\").alias(\"available\"),\n",
    "    sum(\"total\").alias(\"total\"),\n",
    "    sum(\"waitlist\").alias(\"waitlist\"),\n",
    "    sum(\"enrolled_ct\").alias(\"enrolled_ct\"))\n",
    "\n",
    "\n",
    "  # 设置窗口\n",
    "  window_spec = Window.partitionBy(\"subj_course_id\", \"prof\")\n",
    "\n",
    "  # 计算每组中 total 在 [0, 998] 范围内的最大值\n",
    "  # 如果有一个课程的total是-1，说明这个课程的total一直不正常，超出0~998\n",
    "  # 但这个课程可能还是有被正常注册的（即enrolled_ct还在正常增长）\n",
    "  # 需要后续处理时手动辨别过滤\n",
    "  df_with_max = df_temp.withColumn(\n",
    "      \"max_total_valid\",\n",
    "      max(\n",
    "          when(\n",
    "            (col(\"total\") >= 0) & (col(\"total\") <= 998),\n",
    "            col(\"total\")\n",
    "          ).otherwise(-1)\n",
    "      ).over(window_spec)\n",
    "  )\n",
    "\n",
    "  # 替换 total 为该组最大值\n",
    "  df_temp = df_with_max.withColumn(\"total\", col(\"max_total_valid\")).drop(\"max_total_valid\")\n",
    "\n",
    "  return df_temp\n",
    "\n",
    "# 对于每一个 subj_course_id+prof 组合，检查其date是否包含passtime中的全部值\n",
    "# 如果不包含，则删去 subj_course_id+prof 组合\n",
    "def remove_data_not_in_passtime(df, passtime):\n",
    "\n",
    "  # 将需要检查的日期放入set，以供检查\n",
    "  required_dates = set(passtime)\n",
    "  required_dates_count = len(required_dates)\n",
    "\n",
    "  # 使用F.array创建新列，可以直接在表中比较\n",
    "  required_dates_lit = F.array([lit(d) for d in required_dates])\n",
    "\n",
    "  # 搜集唯一日期\n",
    "  grouped_df = df.groupBy(\"subj_course_id\", \"prof\").agg(\n",
    "    F.collect_set(\"date\").alias(\"existing_dates\")\n",
    "  )\n",
    "\n",
    "  # 过滤出符合要求的组合\n",
    "  valid_groups = grouped_df.filter(\n",
    "    F.size(F.array_intersect(F.col(\"existing_dates\"), required_dates_lit)) == required_dates_count\n",
    "  ).select(\"subj_course_id\", \"prof\")\n",
    "\n",
    "  # 仅保留符合要求的组合\n",
    "  df = df.join(valid_groups, [\"subj_course_id\", \"prof\"], \"inner\")\n",
    "\n",
    "  return df\n",
    "\n",
    "def rename_split_delete_columns(df):\n",
    "  df = df.withColumn(\"year\", lit(year))\n",
    "  df = df.withColumn(\"quarter\", lit(quarter))\n",
    "  df = df.withColumn(\"department\", split(\"subj_course_id\", \" \")[0])\n",
    "  df = df.withColumn(\"course_id\", split(\"subj_course_id\", \" \")[1])\n",
    "  df = df.drop(\"subj_course_id\")\n",
    "  df = df.drop(\"available\")\n",
    "  return df\n",
    "\n",
    "# 整体清理函数\n",
    "def clean_data(df, year, quarter, passtime):\n",
    "  df = remove_null_values(df)\n",
    "  df = transfer_timestamp(df)\n",
    "  df = remove_weird_values(df)\n",
    "  df = uniform_prof(df)\n",
    "  df = aggerate_value(df)\n",
    "  df = remove_data_not_in_passtime(df, passtime)\n",
    "  df = rename_split_delete_columns(df)\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdac6778-f067-4ccc-b470-221adcbccc59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Clean raw data for each quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43cf4d4b-2e7f-4089-a37f-2a8bbd9e4d36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(year_quarter)):\n",
    "# for i in [0]:\n",
    "  year = year_quarter[i].split('-')[0]\n",
    "  quarter = year_quarter[i].split('-')[1]\n",
    "  passtime = passtimes[i]\n",
    "  \n",
    "  path = f\"{base_path_raw}/{year}{quarter}/\"\n",
    "  df = spark.read.csv(\n",
    "    path,\n",
    "    header=True,       # 告诉Spark CSV文件包含头部行\n",
    "    inferSchema=True   # 告诉Spark自动推断列的数据类型\n",
    "  )\n",
    "\n",
    "  output_path = f\"{base_path_cleaned}/{year}{quarter}/\"\n",
    "  df = clean_data(df, year, quarter, passtime)\n",
    "  df.coalesce(1).write.csv(output_path, mode='overwrite', header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503ced45-e8ad-4835-a90e-cbbe1f2d6188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Combine the cleaned data into a final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "373e9423-fef1-4e14-a18b-22fcd19ed9cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 整合output_path下之前存储在每个文件夹里的csv文件\n",
    "# 整合为一个单独的csv文件（或者orc），储存到f\"{base_path}/final\"下\n",
    "def consolidate_data_files(base_path: str, output_path: str, output_format: str = 'csv'):\n",
    "    \"\"\"\n",
    "    扫描基础路径下的所有子目录，整合数据文件，并保存到新位置。\n",
    "\n",
    "    :param base_path: 包含各年份季度子文件夹的根路径 (例如 './UCSD_output_csv/')。\n",
    "    :param output_format: 输出格式，可以是 'csv' 或 'orc' (推荐)。\n",
    "    \"\"\"\n",
    "    print(\"--- [Function 1: Consolidate Files] ---\")\n",
    "\n",
    "    # 1. 使用通配符(*)读取所有子目录下的数据文件\n",
    "    #    这个路径会匹配 base_path/2023_Q1/, base_path/2023_Q4/ 等所有目录\n",
    "    input_glob_path = os.path.join(base_path, \"*\", \"\")\n",
    "    print(f\"Reading data from path pattern: {input_glob_path}\")\n",
    "\n",
    "    # 假设源文件是CSV格式\n",
    "    # inferSchema=True 在大数据集上可能较慢，生产环境建议手动定义Schema\n",
    "    df = spark.read.csv(input_glob_path, header=True, inferSchema=True)\n",
    "\n",
    "    # 可选：如果你想知道每个文件来自哪个源文件夹，可以添加一列\n",
    "    # df = df.withColumn(\"source_directory\", input_file_name())\n",
    "\n",
    "    print(f\"Successfully read and consolidated {df.count()} rows.\")\n",
    "\n",
    "    # 2. 定义最终输出路径\n",
    "    final_output_path = os.path.join(output_path, \"final\")\n",
    "    print(f\"Preparing to write data to: {final_output_path}\")\n",
    "\n",
    "    # 3. 根据指定格式写入数据\n",
    "    #    coalesce(1) 将所有分区合并为1个，确保输出为单个文件（在'final'目录下）\n",
    "    if output_format.lower() == 'csv':\n",
    "        df.coalesce(1).write.csv(final_output_path, mode='overwrite', header=True)\n",
    "        print(f\"Successfully wrote consolidated data as CSV.\")\n",
    "    elif output_format.lower() == 'orc':\n",
    "        # ORC是列式存储，性能更优，自带Schema，不需要header选项\n",
    "        df.coalesce(1).write.orc(final_output_path, mode='overwrite')\n",
    "        print(f\"Successfully wrote consolidated data as ORC.\")\n",
    "    else:\n",
    "        print(f\"Error: Unsupported output format '{output_format}'. Please choose 'csv' or 'orc'.\")\n",
    "\n",
    "    print(\"--- [Function 1: Finished] ---\\n\")\n",
    "    return final_output_path\n",
    "\n",
    "# 对最终数据库进行采样，约15%，以JSON格式储存，供前端使用\n",
    "def sample_and_save_as_json(consolidated_data_path: str, base_path: str, input_format: str = 'csv'):\n",
    "    \"\"\"\n",
    "    读取整合后的数据，进行随机采样，并将结果保存为JSON格式。\n",
    "\n",
    "    :param consolidated_data_path: 整合后数据文件的完整路径 (例如 './UCSD_output_csv/final')。\n",
    "    :param base_path: 用于创建 'json_sample' 目录的根路径。\n",
    "    :param input_format: 输入文件的格式，'csv' 或 'orc'。\n",
    "    \"\"\"\n",
    "    print(\"--- [Function 2: Sample and Save as JSON] ---\")\n",
    "    print(f\"Reading consolidated data from: {consolidated_data_path}\")\n",
    "\n",
    "    # 1. 根据指定格式读取整合后的数据\n",
    "    if input_format.lower() == 'csv':\n",
    "        df = spark.read.csv(consolidated_data_path, header=True, inferSchema=True)\n",
    "    elif input_format.lower() == 'orc':\n",
    "        df = spark.read.orc(consolidated_data_path)\n",
    "    else:\n",
    "        print(f\"Error: Unsupported input format '{input_format}'.\")\n",
    "        return\n",
    "\n",
    "    # 2. 对 DataFrame 进行 15% 的不放回随机采样\n",
    "    #    这是一个近似值，结果可能略有浮动\n",
    "    fraction_to_sample = 0.15\n",
    "    df_sampled = df.sample(withReplacement=False, fraction=fraction_to_sample)\n",
    "\n",
    "    print(f\"Original row count: {df.count()}, Sampled row count: {df_sampled.count()}\")\n",
    "\n",
    "    # 3. 定义JSON输出路径\n",
    "    json_output_path = os.path.join(base_path, \"json_sample\")\n",
    "    print(f\"Preparing to write JSON sample to: {json_output_path}\")\n",
    "\n",
    "    # 4. 将采样后的数据以JSON格式写入\n",
    "    #    同样使用 coalesce(1) 输出单个文件\n",
    "    df_sampled.coalesce(1).write.json(json_output_path, mode='overwrite')\n",
    "\n",
    "    print(\"Successfully wrote sampled data as JSON.\")\n",
    "    print(\"--- [Function 2: Finished] ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30bca5ed-424c-4c9c-a727-2caffd0895ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Function 1: Consolidate Files] ---\nReading data from path pattern: s3a://ucsd-registration-s3-20250609193613565500000001/ucsd/cleaned/*/\nSuccessfully read and consolidated 236880 rows.\nPreparing to write data to: s3a://ucsd-registration-s3-20250609193613565500000001/ucsd/final/final\nSuccessfully wrote consolidated data as CSV.\n--- [Function 1: Finished] ---\n\n--- [Function 2: Sample and Save as JSON] ---\nReading consolidated data from: s3a://ucsd-registration-s3-20250609193613565500000001/ucsd/final/final\nOriginal row count: 236880, Sampled row count: 35592\nPreparing to write JSON sample to: s3a://ucsd-registration-s3-20250609193613565500000001/ucsd/final/json_sample\nSuccessfully wrote sampled data as JSON.\n--- [Function 2: Finished] ---\n\n"
     ]
    }
   ],
   "source": [
    "# 整合各季度的数据为一个单独的csv文件，存储到base_path_final\n",
    "consolidate_file_path = consolidate_data_files(base_path_cleaned, base_path_final)\n",
    "\n",
    "# 随机采样15%作为JSON\n",
    "sample_and_save_as_json(consolidate_file_path, base_path_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b709e90-a278-4d56-bcf3-409d0c504dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conculsion\n",
    "Now we have the following cleaned data:\n",
    "- Cleaned registration data for each quarter: {base_path}/cleaned/yearQuarter\n",
    "- Cleaned combined registration data for all quarters: {base_path}/final/final/\n",
    "- Passtime and tags for each quarter: {base_path}/final/passtimes.json"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Cleaning 2025-06-09 11:57:32",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}