{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71a408cb-6458-49a6-9b5a-f04bcd0a4636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Final Table\n",
    "清理后的数据中，课程信息和注册信息混杂在一起。这不仅带来了冗余，也带来了一些麻烦，例如：\n",
    "- 未来我们想要实现按教授名索引课程的功能，然而，部分课程由多个教授任课，这给我们分裂不同课程带来了麻烦。\n",
    "- 未来我们可能会加入关于不同季度课程的评分数据，如果添加进当前表中会使其非常冗杂。\n",
    "\n",
    "因此，我们决定将当前清理过的表分为四张表：\n",
    "- Professors\n",
    "  - 存储教授信息\n",
    "  - `prof_id`: 主键\n",
    "  - `prof_last_name`: 教授的姓, 不可为null\n",
    "  - `prof_first_name`: 教授的名, 可为null\n",
    "  - `prof_middle_name`: 中间名, 可谓null\n",
    "\n",
    "- Courses\n",
    "  - 存储课程信息\n",
    "  - `course_offering_id`: 主键\n",
    "  - `department`: 部门\n",
    "  - `course_id`: 课程编号\n",
    "  - `instructor`: 关键属性，讲师就是原表的 `prof` 列，一门课的讲师是识别其的重要属性之一。\n",
    "    - 教授表只包含单独的教授信息\n",
    "    - 原表中的课程有 `department`, `course_id` 等信息\n",
    "    - 多个教授可能教授多门课程，同一门 `department-course_id` 可能同时开了多门课，由不同教授教授\n",
    "    - 因此，如果没有 `instructor` 列，那么多门课 (`department-course_id-instructor`) 就会被认定为一门课 ((`department-course_id`))，被连接至所有教授这门课的教授\n",
    "  - `year`: 学年\n",
    "  - `quarter`: 季度\n",
    "  - `total`: 总座位数\n",
    "\n",
    "- Course_Professors\n",
    "  - 链接表，链接课程和教授。为什么不做列？：因为同一门课可能有多个教授。\n",
    "  - `course_offering_id`: 外键，连接到 Course 表\n",
    "  - `prof_id`: 外键，链接到 Professors 表\n",
    "\n",
    "- Enrollment_Snapshots\n",
    "  - 注册数据快照表\n",
    "  - `course_offering_id`: 外键，连接到 Course 表\n",
    "  - `date`: 日期\n",
    "  - `enrolled_ct`: 注册人数\n",
    "  - `waitlist`: 候补名单人数\n",
    "\n",
    "在未来，我们还可能添加 Comments 表，Course_Rating 表, Professor_Rating 表，Department_Requirement 表等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94acf53a-16ec-4d7f-b4a7-58db92e066d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "449becda-8250-4dc3-8151-878452ad0de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Required parameters and test S3 connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "788bd697-f7ff-47bc-83c7-26fd875927ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, sum, when, lit\n",
    "from pyspark.sql.functions import sum, avg, max, min, count, countDistinct, first, last, mean, stddev, collect_list, collect_set, approx_count_distinct, expr\n",
    "from pyspark.sql.functions import col, from_unixtime, to_timestamp, date_format, row_number\n",
    "from pyspark.sql.functions import split, locate, explode, trim, substring, size\n",
    "from pyspark.sql.functions import sha2, concat_ws\n",
    "\n",
    "\n",
    "# 必要的参数，链接 AWS S3\n",
    "AWS_ACCESS_KEY = \"\"\n",
    "AWS_SECRET_KEY = \"\"\n",
    "BUCKET_NAME = \"\"\n",
    "REGION = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf7fed1d-b735-472d-9e11-b05d9fb4cd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", AWS_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22a87105-d094-47d7-ba94-22c1be3eeef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+--------+-----------+----+-------+----------+---------+\n|                prof|      date|total|waitlist|enrolled_ct|year|quarter|department|course_id|\n+--------------------+----------+-----+--------+-----------+----+-------+----------+---------+\n|Solomon; Amanda L...|2025-01-03|   -1|       0|          5|2025| Winter|      AAPI|      198|\n|Solomon; Amanda L...|2025-01-02|   -1|       0|          5|2025| Winter|      AAPI|      198|\n|Solomon; Amanda L...|2025-01-18|   -1|       0|          7|2025| Winter|      AAPI|      198|\n+--------------------+----------+-----+--------+-----------+----+-------+----------+---------+\nonly showing top 3 rows\n\nroot\n |-- prof: string (nullable = true)\n |-- date: date (nullable = true)\n |-- total: integer (nullable = true)\n |-- waitlist: integer (nullable = true)\n |-- enrolled_ct: integer (nullable = true)\n |-- year: integer (nullable = true)\n |-- quarter: string (nullable = true)\n |-- department: string (nullable = true)\n |-- course_id: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# 关于S3的基本参数\n",
    "base_path = f\"s3a://{BUCKET_NAME}/ucsd\"\n",
    "path_final_data = f\"{base_path}/final/final\"\n",
    "path_final_table = f\"{base_path}/final_table\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.csv(f\"{path_final_data}\", header=True, inferSchema=True)\n",
    "    display(df.show(3))\n",
    "    df.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"Table read failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0051a075-dd08-4c17-802e-79bd83856c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Tool functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dbd9b1c-cf22-4d64-a0d5-630755f0aa48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# uuid 生成函数\n",
    "# 弃用\n",
    "# 原因：uuid() 会在每次调用时运行一次，返回一个独立的值\n",
    "# 然而，由于Spark惰性求值+不可变的特性，每次我们调用 withColumn(...) 或其他转换操作时，不会修改原始 DataFrame，而是返回一个新的 DataFrame\n",
    "# 这个新的df会重新运行一次uuid生成函数\n",
    "# 最后造成uuid在两个df中不同\n",
    "uuid_udf = F.udf(lambda: str(uuid.uuid4()), StringType())\n",
    "\n",
    "# key 生成函数\n",
    "def generate_id(df, columns, key_col_name):\n",
    "    df = df.withColumn(key_col_name, sha2(concat_ws(\"||\", *columns), 256))\n",
    "    return df\n",
    "\n",
    "# 分裂 prof 为first_name, last_name, middle_name\n",
    "def split_prof_name(df):\n",
    "\n",
    "    # 分裂多个教授授课的课程\n",
    "    # 有一些课程由多个教授授课，这种情况下教授名字被 & 链接\n",
    "    # 例如 Bafna; Vineet & Zhong; Sheng \n",
    "    # 将其分裂为多行\n",
    "    df = df.withColumn(\"prof\", explode(split(col(\"prof\"), \"& \"))) \\\n",
    "            .withColumn(\"prof\", trim(col(\"prof\")))\n",
    "\n",
    "    # 对于professor的名字，其格式为 last_name; first_name middle_name(可能为null)\n",
    "    # 将其分裂为三列\n",
    "    # 如果没有中间名，prof_middle_name 列为null\n",
    "    # 如果教授为 Staff，prof_first_name 和 prof_last_name 都为 Staff\n",
    "\n",
    "    df = df.withColumn(\"isStaff\", col(\"prof\") == lit(\"Staff\"))\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"prof_last_name\",\n",
    "        when(col(\"isStaff\"), \"Staff\")\n",
    "        .otherwise(trim(split(col(\"prof\"), \"; \").getItem(0)))\n",
    "    ).withColumn(\n",
    "        \"first_middle_name\",\n",
    "        when(col(\"isStaff\"), \"Staff\")\n",
    "        .otherwise(trim(split(col(\"prof\"), \"; \").getItem(1)))\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"prof_first_name\", \n",
    "        when(col(\"isStaff\"), \"Staff\")\n",
    "        .otherwise(split(col(\"first_middle_name\"), \" \", 2).getItem(0))\n",
    "    ).withColumn(\n",
    "        \"prof_middle_name\",\n",
    "        when(col(\"isStaff\"), lit(None))\n",
    "        .otherwise(\n",
    "            # 检查有没有middlename\n",
    "            when(size(split(col(\"first_middle_name\"), \" \")) > 1, split(col(\"first_middle_name\"), \" \", 2).getItem(1))\n",
    "            .otherwise(lit(None))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    df = df.drop(\"isStaff\", \"first_middle_name\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# 准备 professors 表的数据\n",
    "def create_prof_table_data(df_original):\n",
    "    df = df_original\n",
    "    df = df.select(\"prof\").distinct()\n",
    "    \n",
    "    # 分裂教授名字\n",
    "    df = split_prof_name(df)\n",
    "    df = df.drop(\"prof\").distinct()\n",
    "\n",
    "    # 生成主键列\n",
    "    df = generate_id(df, [\"prof_first_name\", \"prof_last_name\", \"prof_middle_name\"], \"prof_id\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# 准备 Courses 表的数据\n",
    "def create_courses_table_data(df):\n",
    "    df = df.select(\"department\", \"course_id\", \"year\", \"quarter\", \"total\", \"prof\").distinct()\n",
    "    # 生成主键\n",
    "    df = generate_id(df, [\"department\", \"course_id\", \"year\", \"quarter\", \"total\", \"prof\"], \"course_offering_id\")\n",
    "    return df\n",
    "\n",
    "# 建立 Courses-Professors 连接表\n",
    "# 两张表之间暂时的JOIN列为 prof 列\n",
    "def create_courses_professors_table_data(courses, professors, registrations_original):\n",
    "    \n",
    "    # 现在多了 prof_first_name, prof_last_name, prof_middle_name 列\n",
    "    df = split_prof_name(registrations_original)\n",
    "\n",
    "    # 获取 prof_id\n",
    "    df = df.join(professors, on=[\"prof_first_name\", \"prof_last_name\", \"prof_middle_name\"], how=\"inner\")\n",
    "\n",
    "    # 获取 courses_id\n",
    "    df = df.join(courses, on=[\"year\", \"quarter\", \"department\", \"course_id\", \"total\"], how=\"inner\")\n",
    "\n",
    "    # 只取需要的两列\n",
    "    df = df.select(\"prof_id\", \"course_offering_id\").distinct()\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_enrollment_snapshots_table_data(registrations_original, courses):\n",
    "    df = registrations_original\n",
    "    \n",
    "    # 获取 courses_id\n",
    "    df = df.join(courses, on=[\"year\", \"quarter\", \"department\", \"course_id\", \"prof\", \"total\"], how=\"inner\")\n",
    "\n",
    "    # 只取需要的列\n",
    "    df = df.select(\"date\", \"waitlist\", \"enrolled_ct\", \"course_offering_id\").distinct()\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_tables(professors, courses, courses_professors, enrollment_snapshots):\n",
    "\n",
    "    # professors = professors.selectExpr(\"\")\n",
    "    courses = courses.withColumnRenamed(\"prof\", \"instructor\")\n",
    "\n",
    "    return professors, courses, courses_professors, enrollment_snapshots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07956ad5-3658-474f-bbd0-fabb40572453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_final = spark.read.csv(f\"{path_final_data}\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d876c91-125c-4288-9f06-287966ce0c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+----------------+--------------------+\n|prof_last_name|prof_first_name|prof_middle_name|             prof_id|\n+--------------+---------------+----------------+--------------------+\n|           Som|        Brandon|               D|29d8e421bec3ce8b9...|\n|  Sanchez Cruz|          Jorge|            null|42c8dd041bb069f27...|\n|    Schurmeier|       Kimberly|            null|d3dc139bd6cc4c799...|\n+--------------+---------------+----------------+--------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_professors = create_prof_table_data(data_final)\n",
    "df_professors.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f86a6485-75a4-41dd-9c77-44e7b4d94502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----+-------+-----+--------------------+--------------------+\n|department|course_id|year|quarter|total|                prof|  course_offering_id|\n+----------+---------+----+-------+-----+--------------------+--------------------+\n|       AAS|       11|2025| Winter|   68|Butler; Elizabeth...|f18fb01db40425f1d...|\n|      AESE|      241|2025| Winter|   35|Erat; Sanjiv & Wa...|8ef5ad892185ad15a...|\n|       AAS|       10|2025| Winter|   68|Butler; Elizabeth...|c85a26fe19ff326df...|\n+----------+---------+----+-------+-----+--------------------+--------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_courses = create_courses_table_data(data_final)\n",
    "df_courses.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f4ad4e5-685e-4c74-a552-b0098ac4f823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n|             prof_id|  course_offering_id|\n+--------------------+--------------------+\n|8fdf7dc7d4b9cbfd3...|86dbdaa7645e34173...|\n|8fdf7dc7d4b9cbfd3...|c523e25947201a453...|\n|8fdf7dc7d4b9cbfd3...|fc10273071b29d19e...|\n+--------------------+--------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_courses_professors = create_courses_professors_table_data(df_courses, df_professors, data_final)\n",
    "df_courses_professors.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "647fbd8e-8ae5-405c-9534-b5ee38fb385b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+--------------------+\n|      date|waitlist|enrolled_ct|  course_offering_id|\n+----------+--------+-----------+--------------------+\n|2024-11-13|       0|          2|6d1f02cafc5ab40a2...|\n|2024-11-15|       0|          2|6d1f02cafc5ab40a2...|\n|2024-11-20|       0|          2|6d1f02cafc5ab40a2...|\n+----------+--------+-----------+--------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_enrollment_snapshots = create_enrollment_snapshots_table_data(data_final, df_courses)\n",
    "df_enrollment_snapshots.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f2e7e0-7feb-4759-b7e2-f3e01d4d12e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final clean\n",
    "df_professors, df_courses, df_courses_professors, df_enrollment_snapshots = clean_tables(df_professors, df_courses, df_courses_professors, df_enrollment_snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca7b356-4590-4da0-8e68-a1890049ce6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[42]: 13467"
     ]
    }
   ],
   "source": [
    "df_enrollment_snapshots.join(df_courses, on=\"course_offering_id\", how=\"inner\").select(\"department\", \"course_id\", \"year\", \"quarter\", \"instructor\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b92fe8e1-36ff-4a31-a2cf-74cf4afbe954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[44]: 13467"
     ]
    }
   ],
   "source": [
    "data_final.select(\"department\", \"course_id\", \"year\", \"quarter\", \"prof\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ffc72ba-c2f0-4ca9-af94-e227a7be18f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Store the table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8abfc8dd-1836-4336-a3bb-3c8f43b903a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path_table_professors = f\"{path_final_table}/professors\"\n",
    "path_table_courses = f\"{path_final_table}/courses\"\n",
    "path_table_courses_professors = f\"{path_final_table}/courses_professors\"\n",
    "path_table_enrollment_snapshots = f\"{path_final_table}/enrollment_snapshots\"\n",
    "\n",
    "# write to s3\n",
    "df_professors.coalesce(1).write.csv(path_table_professors, mode=\"overwrite\", header=True)\n",
    "df_courses.coalesce(1).write.csv(path_table_courses, mode=\"overwrite\", header=True)\n",
    "df_courses_professors.coalesce(1).write.csv(path_table_courses_professors, mode=\"overwrite\", header=True)\n",
    "df_enrollment_snapshots.coalesce(1).write.csv(path_table_enrollment_snapshots, mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75e5360f-6f24-456d-98c5-134d464777cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "现在我们有四个数据集，分别储存在 `{base_path}/final_table/` 下的文件夹中。"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Table Creation 2025-06-11 10:32:55",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}